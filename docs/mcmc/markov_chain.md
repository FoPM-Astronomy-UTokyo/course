# 動的なモンテカルロ法

[前項](./static_monte_carlo.md) では問題の次元が大きくなると, 静的なモンテカルロ法ではターゲットとなる分布にヒットする確率が著しく下がるため, 効率よく計算することができないという問題について触れました.

これはターゲットとなる確率分布を提案分布でうまく近似できていないことに起因します. そこで, 確率分布全体をうまく近似することをあきらめて, あるデータの近傍でのみうまく近似することを考えます. ごく近傍でしかつじつまが合っていなくても, その情報を適切につなぎ合わせることができれば, 確率分布全体をうまく表現することができるはずです.

ここでは「マルコフ連鎖モンテカルロ法」について説明します. データを効率よくサンプリングするために, あるデータの近傍でのみ次のサンプルを探すという戦略をとります. 一回の操作では確率分布関数のごく一部しか探索することができませんが, この操作を多数回繰り返すことによって確率分布関数全体の形状をを浮かび上がらせることができます.


## マルコフ連鎖とは

マルコフ連鎖とはマルコフ過程によって生成された状態 (データ) の列 $\{x_t\}_{t=0{\ldots}}$ を表します.[^1] マルコフ過程とは "現在の状態 $x$ から次の状態 $x'$ へ遷移する確率 $\pi(x \to x')$ が現在の状態によって決まる" という過程を表しています.[^2]

[^1]: 時系列っぽさを意識してなんとなく index を $t$ にしていますが, あまり深い意味はありません.

[^2]: 時系列データをモデリングする場合 (状態空間モデル) などでは $x_t$ から $x_{t+1}$ への遷移確率を定める "現在の状態" として $\tilde{x}_t = (x_{t-2},x_{t-1},x_{t})^T$ のように過去の情報を含めることもあります.

ある条件のもとでは十分に長い時間遷移を繰り返したときの $\{x_t\}_{t=0{\ldots}}$ の分布が (初期値 $x_0$ に依らず) ある分布 $\mathcal{P}(x)$ に収束することが期待されます.[^3] この分布のことを定常分布と呼びます. ある時刻 $t$ までマルコフ連鎖を繰り返したときに期待される $x$ の確率分布関数を $\mathcal{P}_t(x)$ とします. ここから時刻 $t+1$ における分布 $\mathcal{P}_{t+1}(x)$ を計算すると以下の式になります.[^4]

$$
\mathcal{P}_{t+1}(x') = \int\mathrm{d}x\,\mathcal{P}_{t}(x)\pi(x \to x').
$$

[^3]: 乱数をサンプリングしたい確率分布関数 $P(x)$ との混同を避けるために, マルコフ連鎖によって生成されるデータ列に期待される確率分布関数を $\mathcal{P}(x)$ とフォントを少し変えて記述しています.

[^4]: 連続分布として書いていますが $x$ が離散的な場合は和に置き換えてください.

$t \to \infty$ のときに $\mathcal{P}_{t}(x)$ が定常分布に収束するのであれば $\mathcal{P}(x)$ は,

$$
\mathcal{P}(x') = \int\mathrm{d}x\,\mathcal{P}(x)\pi(x \to x').
$$

を満たしていることが必要になります. また, こうした定常分布が存在するためには, 状態遷移のルールにも制約があります. まず, 存在するどの状態ペア $(x, x')$ も有限回の操作で遷移できる (状態が分断されていない) ことが必要です. 加えて, 状態の遷移には非周期性も必要とされますが, ここでは詳細については省略します.[^5]

[^5]: チェッカーボードの上を 1 マスずつ移動するコマを考えてみます. コマの分布はこの場合 $t$ が奇数のときと偶数のときでそれぞれ定常分布に収束しますが, 全体で見たときは $\mathcal{P}_\mathrm{odd}(x)$, $\mathcal{P}_\mathrm{even}(x)$ で振動してしまい収束しません. 非周期性はこうしたケースを排除するために要求されます.

十分に長い時間マルコフ連鎖のステップを繰り返すことで, マルコフ連鎖列 $\{x_t\}_{t=0{\ldots}}$ を定常分布 $\mathcal{P}(x)$ からサンプルした乱数とみなすことができます. ここで状態遷移のルールをうまく設定することによって, 収束先の定常分布をコントロールすることができます. ターゲットとなる確率分布関数 $P(x)$ に収束するようなマルコフ連鎖を設計することができれば, 任意の確率分布関数からの乱数を効率よくサンプリングすることができそうです.

??? tip "分布の収束性についてのコメント"
    ここでは操作を繰り返すことによって期待される分布がどのように変わっていくかを考えることで, 分布の収束性について説明します. $t=0$ のときに期待される $x$ の分布を $\mathcal{P}_0{x}$ とします. これは初期状態として定義しているだけで任意の分布です. 1 ステップ後の分布を以下のように書くことができます.

    $$
    \mathcal{P}_1(x) = \int\mathrm{d}x'\,\mathcal{P}_0(x')\pi(x' \to x)
    = c\mathcal{P}(x) + (1-c)R_0(x).
    $$

    ただし $0 < c < 1$ で $\mathcal{P}_0(x)$ に依らない定数になります (この形で書けることの証明は省略します). $\pi(x \to x')$ の操作をすることによって $\mathcal{P}_1(x)$ から $c$ 倍だけ $\mathcal{P}(x)$ を削り出すことができます. 同じ状態遷移を実行すると,

    $$
    \begin{aligned}
    \mathcal{P}_2(x) &= c\mathcal{P}(x)
                        + (1-c)(c\mathcal{P}(x) + (1-c)R_1(x)) \\
                     &= \left(1 - (1-c)^2\right) \mathcal{P}(x)
                        + (1-c)^2 R_1(x)
    \end{aligned}
    $$

    と変換できます. ただし $R_0(x)$ を初期分布として $c\mathcal{P}(x)$ を削りだす操作を再びおこっています. 状態遷移を $m$ 回繰り返すことによって以下の式を得ます.

    $$
    \mathcal{P}_m(x) = \left(1 - (1-c)^m\right) \mathcal{P}(x)
                       + (1-c)^m R_{m-1}(x).
    $$

    $m \to \infty$ の極限で任意の分布 $\mathcal{P}_0(x)$ から $\mathcal{P}(x)$ に収束することが示せました.


## マルコフ連鎖によるサンプル生成

ここまで, マルコフ連鎖によって定常分布 $\mathcal{P}(x)$ に従う乱数を生成することができることを確認しました. 次に定常分布 $\mathcal{P}(x)$ を任意の確率分布関数 $P(x)$ にするための方法について紹介します. 定常分布が満たすべき式は

$$
\mathcal{P}(x') = \int\mathrm{d}x\,\mathcal{P}(x)\pi(x \to x')
$$

です. この式が $\mathcal{P}(x) \gets P(x)$ で成立する遷移確率 $\pi(x \to x')$ を設計するためには, あらゆる遷移について矛盾が無いようにつじつまを合わせる必要があります. これを満たす遷移を考えるのはかなり難しいので, 上記のつりあいを満たすための十分条件として以下を考えます.

$$
\mathcal{P}(x')\pi(x' \to x) = \mathcal{P}(x)\pi(x \to x').
$$

これを詳細つりあい条件と呼びます. 両辺を $x$ について積分すれば元の式に戻るため, 十分条件として成立していることは自明です. 任意の $(x, x')$ について

$$
{P}(x')\pi(x' \to x) = {P}(x)\pi(x \to x')
$$

が成り立つように状態遷移を設計することによってマルコフ連鎖から $P(x)$ に従う乱数をサンプリングすることができます. そのための手法のひとつが Metropolis-Hastings アルゴリズムです.


### Metropolis-Hastings algorithm

Metropolis-Hastings アルゴリズムは以下の手順に従ってマルコフ連鎖を生成します.

1. 現在の状態 $x_t$ から次の状態の候補 $x_p$ を確率分布 $Q(x_p; x_t)$ からサンプリングする.[^6]
1. $P(x)$, $Q(x_p; x)$ から以下の量を計算する.[^7]

$$
\alpha = \frac{P(x_p)}{P(x_t)}\frac{Q(x_t;x_p)}{Q(x_p; x_t)}
$$

3. $[0,1)$ の一様分布から乱数 $u$ をサンプリングする.
    1. $u \leq \alpha$ であれば提案された状態 $x_p$ を $x_{t+1}$ として採用する.
    1. $u > \alpha$ であれば現在の状態 $x_t$ を $x_{t+1}$ として採用する.

[^6]: $x$ の近傍をサンプルしたいので $x$ をパラメタとして持ちます. 確率分布ではありますが物理的に意味があるわけではない (あるとは限らない) ので文字に $Q$ を使いました.

[^7]: $\alpha$ の計算では比をとるので $P(x)$ の規格化定数は未知のままで問題ありません.


このように状態遷移を定義することによって, 確率分布 $P(x)$ に従う乱数をサンプリングできます. ただし, 次の状態を提案する確率分布 $Q(x_p; x)$ の選択によっては出力されるデータ列 $\{x_t\}_{t=0{\ldots}}$ の分布が $P(x)$ へと収束するまでに数多くの状態遷移が必要になります. 状態 $x$ の近傍を探索するためには, 提案分布 $Q(x_p; x)$ は $x$ を中心とした多次元正規分布のような分布が多く採用されます. この提案分布のステップ幅が広すぎると静的なモンテカルロ法で直面した問題が再燃して収束が遅くなります. 一方で, ステップ幅が狭すぎても確率分布を覆い尽くすまでに時間がかかるために収束が遅くなります. 提案分布の選択はある程度試行錯誤しながら決める必要があります.


??? tip "Metropolis-Hastings アルゴリズムが詳細つりあい条件を満たすことの確認"
    まずは $\alpha \leq 1$ のケースを考えます. $x_t \to x_p$ へと遷移する確率 $\pi(x_t \to x_p)$ は $Q(x_p; x_t)$ から $x_p$ が選ばれ, かつ確率 $\alpha$ で採択されるという条件になります. 一方, $x_p \to x_t$ へと遷移する過程では $\alpha$ に相当する量がかならず 1 を超える (逆数になる) ので以下のように書けます.

    $$
    \left\{~\begin{aligned}
    \pi(x_t \to x_p) &= \alpha Q(x_p; x_t) \\
    \pi(x_p \to x_t) &= Q(x_t; x_p)
    \end{aligned}\right.,
    $$

    よって $P(x_t)\pi(x_t \to x_p)$ を計算すると,

    $$
    P(x_t)\pi(x_t \to x_p)
    = P(x_t) \frac{P(x_p)}{P(x_t)}\frac{Q(x_t; x_p)}{Q(x_p; x_t)}Q(x_p; x_t)
    = P(x_p)Q(x_t; x_p)
    $$

    となります. 同様に $P(x_p)\pi(x_p \to x_t)$ を計算すると,

    $$
    P(x_p)\pi(x_p \to x_t) = P(x_p)Q(x_t; x_p)
    $$

    となるため, 詳細つりあい条件を満たしていることが分かります. $\alpha > 1$ の場合も同様に示すことができます.


### Gibbs sampling

Metropolis-Hastings アルゴリズムの特殊な例に Gibbs サンプリングという手法があります. 多次元の確率密度分布 $P(x)$ 全体を一度にモデル化することは難しいとしても, ある 1 次元だけ, あるいはある部分空間だけを抜き出した条件付き分布だけは厳密に求められるという場合があります. Gibbls サンプリングでは以下のようにしてマルコフ連鎖を生成します.

1. 現在の状態 $x_t$ から更新する次元 $i$ をランダムに選択する.[^8]
1. 次の状態 $x_p$ を $P(x_t^{(i)}|x_t^{({\neg}\,i)})$ からサンプルする.

[^8]: 必ずしもランダムである必要はなく順番に選んでも問題ありません.

$x^i$, $x^{\neg i}$ はそれぞれ $x$ の $i$ 番目の次元のみの成分と $i$ 番目の次元をを除いた成分を表しています. 次の状態 $x_p$ を選択する提案分布 $Q(x_p; x)$ は以下のように書けます.

$$
Q(x_p; x_t) = P(i)P(x_p^{i}|x_t^{{\neg}i}), \quad
Q(x_t; x_p) = P(i)P(x_t^{i}|x_t^{{\neg}i})
$$

ここで $P(i)$ は $i$ 番目の次元が選ばれる確率です. $\alpha$ の値は以下のように書けます.

$$
\alpha = \frac{P(x_p)}{P(x_t)}
\frac{P(i)P(x_t^{i}|x_t^{{\neg}i})}{P(i)P(x_p^{i}|x_t^{{\neg}i})}
$$

分母分子に $P(x_t^{\neg{i}})$ をかけると $\alpha = 1$ であることが確認できます. Gibbs サンプリングは提案分布に厳密な条件付き確率分布を用いることによって, 必ず採択されることが保証された Metropolis-Hastings アルゴリズムだと解釈することができます.


また, マルコフ連鎖の初期のステップについては初期状態に依存している時期があります. 計算結果が初期状態に依らないことを期待して, 計算を始めてからいくらかの割合を捨て去ることがあります. 初期状態を忘れるまでに費やす期間を warm up と呼んだり, 初期のサンプルを捨て去る作業を burn-in と呼んだりします.
